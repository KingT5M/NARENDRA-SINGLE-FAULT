{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingT5M/NARENDRA-SINGLE-FAULT/blob/main/NARENDRA_SINGLE_FAULT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\T5M\\anaconda3\\envs\\Sankara-Ai\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\T5M\\AppData\\Local\\Temp\\ipykernel_6560\\608937545.py:7: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  import kerastuner as kt\n"
          ]
        }
      ],
      "source": [
        "# Import packages\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import kerastuner as kt\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, BatchNormalization, MaxPooling1D, LSTM, Flatten, Dense, Dropout, Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import Callback\n",
        "# Set paths\n",
        "data_dir = r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET'\n",
        "graphs_path = os.path.join(data_dir, 'graphs')\n",
        "best_model_path = os.path.join(data_dir, 'best_model')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(graphs_path, exist_ok=True)\n",
        "os.makedirs(best_model_path, exist_ok=True)\n",
        "\n",
        "# Read and process data\n",
        "file_paths = [\n",
        "    'rec3_002-rpm-healthy.csv',\n",
        "    'rec3_004-gain-2.csv',\n",
        "    'rec3_007-offset-1000.csv',\n",
        "    'rec3_008-stuck.csv',\n",
        "    'rec3_009-noise.csv',\n",
        "    'rec3_010-drift.csv',\n",
        "    'rec3_010-hard-over.csv',\n",
        "    'rec3_012-delay-2.0.csv',\n",
        "    'rec3_012-spike.csv'\n",
        "]\n",
        "\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(os.path.join(data_dir, file_path))\n",
        "    df['time'] = df['time'].round(3)\n",
        "    threshold_index = df[df['time'] > 300.66].index.min()\n",
        "    if not pd.isnull(threshold_index):\n",
        "        df = df.loc[:threshold_index]\n",
        "    df.to_csv(os.path.join(data_dir, file_path), index=False)\n",
        "\n",
        "for file_path in file_paths:\n",
        "    process_file(file_path)\n",
        "\n",
        "# Concatenate data\n",
        "dfs = [pd.read_csv(os.path.join(data_dir, file_path)) for file_path in file_paths]\n",
        "concatenated_df = pd.concat(dfs, ignore_index=True).dropna().sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Split data\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "train_end = int(len(concatenated_df) * train_ratio)\n",
        "val_end = int(len(concatenated_df) * (train_ratio + val_ratio))\n",
        "\n",
        "train_df = concatenated_df[:train_end]\n",
        "val_df = concatenated_df[train_end:val_end]\n",
        "test_df = concatenated_df[val_end:]\n",
        "\n",
        "# Extract features and labels\n",
        "def extract_features_labels(df):\n",
        "    X = df.iloc[:, :2].values\n",
        "    y = df.iloc[:, 2].values\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = extract_features_labels(train_df)\n",
        "X_val, y_val = extract_features_labels(val_df)\n",
        "X_test, y_test = extract_features_labels(test_df)\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_val_encoded = encoder.transform(y_val.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Create sequences of 30\n",
        "def create_sequences(X, y):\n",
        "    sequence_length = 30\n",
        "    X_sequences = [X[i:i+sequence_length] for i in range(len(X) - sequence_length + 1)]\n",
        "    y_sequences = [y[i+sequence_length-1] for i in range(len(y) - sequence_length + 1)]\n",
        "    return np.array(X_sequences), np.array(y_sequences)\n",
        "\n",
        "X_train_array, y_train_array = create_sequences(X_train_scaled, y_train_encoded)\n",
        "X_val_array, y_val_array = create_sequences(X_val_scaled, y_val_encoded)\n",
        "X_test_array, y_test_array = create_sequences(X_test_scaled, y_test_encoded)\n",
        "# Define metrics callback\n",
        "class MetricsCallback(Callback):\n",
        "    def __init__(self, validation_data, graphs_path):\n",
        "        super(MetricsCallback, self).__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.graphs_path = graphs_path\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        X_val, y_val = self.validation_data\n",
        "        y_pred = self.model.predict(X_val)\n",
        "        y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "        y_true = tf.argmax(y_val, axis=1)\n",
        "\n",
        "        precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "        print(f\"Validation Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "        if precision >= 0.9886 and recall >= 0.9890 and f1 >= 0.9888:\n",
        "            print(\"Achieved desired metrics. Stopping training.\")\n",
        "            self.model.stop_training = True\n",
        "            \n",
        "            # Sensor fault types\n",
        "            fault_types = ['healthy', 'gain', 'offset', 'stuck-at', 'noise', 'drift', 'hard-over', 'delay-time', 'spike']\n",
        "\n",
        "            # Plotting precision, recall, and F1-score\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            bar_width = 0.2\n",
        "            index = list(range(len(fault_types)))\n",
        "            plt.bar(index, precision, bar_width, label='Precision')\n",
        "            plt.bar([i + bar_width for i in index], recall, bar_width, label='Recall')\n",
        "            plt.bar([i + 2 * bar_width for i in index], f1, bar_width, label='F1-score')\n",
        "            plt.xlabel('Fault Types')\n",
        "            plt.ylabel('Scores')\n",
        "            plt.title('Precision, Recall, and F1-score')\n",
        "            plt.xticks([i + bar_width for i in index], fault_types, rotation=45)\n",
        "            plt.legend()\n",
        "            plt.tight_layout()\n",
        "            plt.grid(axis='y')\n",
        "            plt.savefig(os.path.join(self.graphs_path, 'metrics_combined.png'))\n",
        "            plt.close()\n",
        "\n",
        "# Define hypermodel\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    \n",
        "    cnn_layers = hp.Int('cnn_layers', min_value=0, max_value=5, default=5)\n",
        "    lstm_layers = hp.Int('lstm_layers', min_value=0, max_value=5, default=4)\n",
        "    dense_layers = hp.Int('dense_layers', min_value=0, max_value=5, default=0)\n",
        "    epochs = hp.Int('epochs', min_value=50, max_value=900, default=850)\n",
        "    max_pooling = hp.Int('max_pooling', min_value=0, max_value=1, default=1)\n",
        "    dropout = hp.Int('dropout', min_value=0, max_value=2, default=0)\n",
        "    batch_norm = hp.Int('batch_norm', min_value=0, max_value=2, default=2)\n",
        "    batch_size = hp.Int('batch_size', min_value=64, max_value=150, default=64)\n",
        "    learning_rate = hp.Float('learning_rate', min_value=0.0001, max_value=0.001, default=0.0005, sampling='linear')\n",
        "\n",
        "    for i in range(cnn_layers):\n",
        "        model.add(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same', input_shape=(30, 2)))\n",
        "        if batch_norm:\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "    if max_pooling:\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    for i in range(lstm_layers):\n",
        "        model.add(LSTM(units=64, activation='relu', return_sequences=True))\n",
        "        if batch_norm:\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    for i in range(dense_layers):\n",
        "        model.add(Dense(units=64, activation='relu'))\n",
        "        if dropout:\n",
        "            model.add(Dropout(0.5))\n",
        "        if batch_norm:\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(units=9, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define ALL_CLASSES\n",
        "ALL_CLASSES = {\n",
        "    'Activation': Activation,\n",
        "    'Conv1D': Conv1D,\n",
        "    'BatchNormalization': BatchNormalization,\n",
        "    'MaxPooling1D': MaxPooling1D,\n",
        "    'LSTM': LSTM,\n",
        "    'Flatten': Flatten,\n",
        "    'Dense': Dense,\n",
        "    'Dropout': Dropout\n",
        "}\n",
        "\n",
        "# Define REINFORCE algorithm\n",
        "def reinforce_update(current_hp, rewards, learning_rate=0.001):\n",
        "    rewards -= np.mean(rewards)\n",
        "    rewards /= np.std(rewards)\n",
        "    \n",
        "    for hp_name, reward in zip(current_hp.keys(), rewards):\n",
        "        current_hp[hp_name] += learning_rate * reward\n",
        "    \n",
        "    return current_hp\n",
        "\n",
        "# Define parameters\n",
        "num_episodes = 100\n",
        "max_steps_per_episode = 50\n",
        "current_hyperparameters = {\n",
        "    'cnn_layers': 5,\n",
        "    'lstm_layers': 4,\n",
        "    'dense_layers': 0,\n",
        "    'epochs': 850,\n",
        "    'max_pooling': 1,\n",
        "    'dropout': 0,\n",
        "    'batch_norm': 2,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.0005\n",
        "}\n",
        "\n",
        "# Define metrics callback\n",
        "metrics_callback = MetricsCallback((X_val_array, y_val_array), graphs_path)\n",
        "\n",
        "# Training loop with REINFORCE\n",
        "for episode in range(num_episodes):\n",
        "    episode_rewards = []\n",
        "    \n",
        "    for step in range(max_steps_per_episode):\n",
        "        sampled_hyperparameters = {\n",
        "            'cnn_layers': int(np.random.normal(current_hyperparameters['cnn_layers'], 0.1)),\n",
        "            'lstm_layers': int(np.random.normal(current_hyperparameters['lstm_layers'], 0.1)),\n",
        "            'dense_layers': int(np.random.normal(current_hyperparameters['dense_layers'], 0.1)),\n",
        "            'epochs': int(np.random.normal(current_hyperparameters['epochs'], 0.1)),\n",
        "            'max_pooling': int(np.random.normal(current_hyperparameters['max_pooling'], 0.1)),\n",
        "            'dropout': int(np.random.normal(current_hyperparameters['dropout'], 0.1)),\n",
        "            'batch_norm': int(np.random.normal(current_hyperparameters['batch_norm'], 0.1)),\n",
        "            'batch_size': int(np.random.normal(current_hyperparameters['batch_size'], 0.1)),\n",
        "            'learning_rate': np.random.normal(current_hyperparameters['learning_rate'], 0.1)\n",
        "        }\n",
        "        print(\"sampled_hyperparameters:\", sampled_hyperparameters)\n",
        "\n",
        "        # Debugging: Print ALL_CLASSES and sampled_hyperparameters\n",
        "        print(\"ALL_CLASSES:\", ALL_CLASSES)\n",
        "        print(\"sampled_hyperparameters:\", sampled_hyperparameters)\n",
        "\n",
        "        # Create a new instance of HyperParameters\n",
        "        hp = kt.HyperParameters()\n",
        "\n",
        "        # Add configurations to the HyperParameters object\n",
        "        for key, value in sampled_hyperparameters.items():\n",
        "            hp.Fixed(key, value)\n",
        "\n",
        "        # Call from_config() method with the configured HyperParameters object\n",
        "        hp_config = {'space': sampled_hyperparameters}\n",
        "        print(\"config after from_config:\", hp_config)\n",
        "\n",
        "        # Use the configured HyperParameters object when calling build_model()\n",
        "        model = build_model(hp)\n",
        "        \n",
        "        print(\"Model built:\", model)\n",
        "        \n",
        "        history = model.fit(\n",
        "            X_train_array,\n",
        "            y_train_array,\n",
        "            validation_data=(X_val_array, y_val_array),\n",
        "            batch_size=sampled_hyperparameters['batch_size'],\n",
        "            epochs=sampled_hyperparameters['epochs'],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        y_pred = model.predict(X_val_array)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        y_true = np.argmax(y_val_array, axis=1)\n",
        "        precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "        reward = (precision >= 0.9886 and recall >= 0.9890 and f1 >= 0.9888) - 1\n",
        "        \n",
        "        episode_rewards.append(reward)\n",
        "    \n",
        "    current_hyperparameters = reinforce_update(current_hyperparameters, episode_rewards)\n",
        "    print(f\"Episode {episode + 1}: Rewards - {np.mean(episode_rewards)}\")\n",
        "\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = kt.HyperParameters().from_config(current_hyperparameters)\n",
        "\n",
        "# Build the model with the best hyperparameters\n",
        "best_model = build_model(best_hps)\n",
        "\n",
        "# Train the model\n",
        "history = best_model.fit(\n",
        "    X_train_array,\n",
        "    y_train_array,\n",
        "    validation_data=(X_val_array, y_val_array),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    epochs=best_hps.get('epochs'),\n",
        "    callbacks=[metrics_callback]\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save(os.path.join(best_model_path, 'best_model.h5'))\n",
        "\n",
        "# Calculate metrics on the validation set\n",
        "y_pred = best_model.predict(X_val_array)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_val_array, axis=1)\n",
        "\n",
        "precision = precision_score(y_true, y_pred_classes, average=None, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred_classes, average=None)\n",
        "f1 = f1_score(y_true, y_pred_classes, average=None)\n",
        "\n",
        "# Sensor fault types\n",
        "fault_types = ['healthy', 'gain', 'offset', 'stuck-at', 'noise', 'drift', 'hard-over', 'delay-time', 'spike']\n",
        "\n",
        "# Plotting precision, recall, and F1-score\n",
        "plt.figure(figsize=(10, 5))\n",
        "bar_width = 0.2\n",
        "index = list(range(len(fault_types)))\n",
        "plt.bar(index, precision, bar_width, label='Precision')\n",
        "plt.bar([i + bar_width for i in index], recall, bar_width, label='Recall')\n",
        "plt.bar([i + 2 * bar_width for i in index], f1, bar_width, label='F1-score')\n",
        "plt.xlabel('Fault Types')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Precision, Recall, and F1-score')\n",
        "plt.xticks([i + bar_width for i in index], fault_types, rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y')\n",
        "plt.savefig(os.path.join(graphs_path, 'metrics_combined.png'))\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPVnutrJohyyPYjZ76dMzE1",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "0.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
