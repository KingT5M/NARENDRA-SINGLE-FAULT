{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingT5M/NARENDRA-SINGLE-FAULT/blob/main/NARENDRA_SINGLE_FAULT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import kerastuner as kt\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, BatchNormalization, MaxPooling1D, LSTM, Flatten, Dense, Dropout, Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set paths\n",
        "data_dir = r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET'\n",
        "graphs_path = os.path.join(data_dir, 'graphs')\n",
        "best_model_path = os.path.join(data_dir, 'best_model')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(graphs_path, exist_ok=True)\n",
        "os.makedirs(best_model_path, exist_ok=True)\n",
        "\n",
        "# Read and process data\n",
        "file_paths = [\n",
        "    'rec3_002-rpm-healthy.csv',\n",
        "    'rec3_004-gain-2.csv',\n",
        "    'rec3_007-offset-1000.csv',\n",
        "    'rec3_008-stuck.csv',\n",
        "    'rec3_009-noise.csv',\n",
        "    'rec3_010-drift.csv',\n",
        "    'rec3_010-hard-over.csv',\n",
        "    'rec3_012-delay-2.0.csv',\n",
        "    'rec3_012-spike.csv'\n",
        "]\n",
        "\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(os.path.join(data_dir, file_path))\n",
        "    df['time'] = df['time'].round(3)\n",
        "    threshold_index = df[df['time'] > 300.66].index.min()\n",
        "    if not pd.isnull(threshold_index):\n",
        "        df = df.loc[:threshold_index]\n",
        "    df.to_csv(os.path.join(data_dir, file_path), index=False)\n",
        "\n",
        "for file_path in file_paths:\n",
        "    process_file(file_path)\n",
        "\n",
        "# Concatenate data\n",
        "dfs = [pd.read_csv(os.path.join(data_dir, file_path)) for file_path in file_paths]\n",
        "concatenated_df = pd.concat(dfs, ignore_index=True).dropna().sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Split data\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "train_end = int(len(concatenated_df) * train_ratio)\n",
        "val_end = int(len(concatenated_df) * (train_ratio + val_ratio))\n",
        "\n",
        "train_df = concatenated_df[:train_end]\n",
        "val_df = concatenated_df[train_end:val_end]\n",
        "test_df = concatenated_df[val_end:]\n",
        "\n",
        "# Extract features and labels\n",
        "def extract_features_labels(df):\n",
        "    X = df.iloc[:, :2].values\n",
        "    y = df.iloc[:, 2].values\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = extract_features_labels(train_df)\n",
        "X_val, y_val = extract_features_labels(val_df)\n",
        "X_test, y_test = extract_features_labels(test_df)\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_val_encoded = encoder.transform(y_val.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Create sequences of 30\n",
        "def create_sequences(X, y):\n",
        "    sequence_length = 30\n",
        "    X_sequences = [X[i:i+sequence_length] for i in range(len(X) - sequence_length + 1)]\n",
        "    y_sequences = [y[i+sequence_length-1] for i in range(len(y) - sequence_length + 1)]\n",
        "    return np.array(X_sequences), np.array(y_sequences)\n",
        "\n",
        "X_train_array, y_train_array = create_sequences(X_train_scaled, y_train_encoded)\n",
        "X_val_array, y_val_array = create_sequences(X_val_scaled, y_val_encoded)\n",
        "X_test_array, y_test_array = create_sequences(X_test_scaled, y_test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sampled_hyperparameters: {'cnn_layers': 5.189509646166705, 'lstm_layers': 4.005461764511269, 'dense_layers': -0.032989595386170065, 'epochs': 850.0048703204084, 'max_pooling': 1.0815271025526854, 'dropout': -0.1834531357665548, 'batch_norm': 2.2160481300151154, 'batch_size': 64.01970313687808, 'learning_rate': -0.025750921284782576}\n",
            "ALL_CLASSES: {'Activation': <class 'keras.src.layers.core.activation.Activation'>, 'Conv1D': <class 'keras.src.layers.convolutional.conv1d.Conv1D'>, 'BatchNormalization': <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>, 'MaxPooling1D': <class 'keras.src.layers.pooling.max_pooling1d.MaxPooling1D'>, 'LSTM': <class 'keras.src.layers.rnn.lstm.LSTM'>, 'Flatten': <class 'keras.src.layers.reshaping.flatten.Flatten'>, 'Dense': <class 'keras.src.layers.core.dense.Dense'>, 'Dropout': <class 'keras.src.layers.regularization.dropout.Dropout'>}\n",
            "sampled_hyperparameters: {'cnn_layers': 5.189509646166705, 'lstm_layers': 4.005461764511269, 'dense_layers': -0.032989595386170065, 'epochs': 850.0048703204084, 'max_pooling': 1.0815271025526854, 'dropout': -0.1834531357665548, 'batch_norm': 2.2160481300151154, 'batch_size': 64.01970313687808, 'learning_rate': -0.025750921284782576}\n",
            "config after from_config: {'space': {'cnn_layers': 5.189509646166705, 'lstm_layers': 4.005461764511269, 'dense_layers': -0.032989595386170065, 'epochs': 850.0048703204084, 'max_pooling': 1.0815271025526854, 'dropout': -0.1834531357665548, 'batch_norm': 2.2160481300151154, 'batch_size': 64.01970313687808, 'learning_rate': -0.025750921284782576}}\n",
            "WARNING:tensorflow:From c:\\Users\\T5M\\anaconda3\\envs\\Sankara-Ai\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'float' object cannot be interpreted as an integer",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[34], line 152\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig after from_config:\u001b[39m\u001b[38;5;124m\"\u001b[39m, hp_config)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Use the configured HyperParameters object when calling build_model()\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel built:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n\u001b[0;32m    156\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    157\u001b[0m     X_train_array,\n\u001b[0;32m    158\u001b[0m     y_train_array,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    163\u001b[0m )\n",
            "Cell \u001b[1;32mIn[34], line 58\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(hp)\u001b[0m\n\u001b[0;32m     55\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mInt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m, min_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, max_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m     56\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mFloat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, min_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, max_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m, sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcnn_layers\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     59\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(Conv1D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_norm:\n",
            "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
          ]
        }
      ],
      "source": [
        "# Define metrics callback\n",
        "class MetricsCallback(Callback):\n",
        "    def __init__(self, validation_data, graphs_path):\n",
        "        super(MetricsCallback, self).__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.graphs_path = graphs_path\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        X_val, y_val = self.validation_data\n",
        "        y_pred = self.model.predict(X_val)\n",
        "        y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "        y_true = tf.argmax(y_val, axis=1)\n",
        "\n",
        "        precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "        print(f\"Validation Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "        if precision >= 0.9886 and recall >= 0.9890 and f1 >= 0.9888:\n",
        "            print(\"Achieved desired metrics. Stopping training.\")\n",
        "            self.model.stop_training = True\n",
        "            \n",
        "            # Sensor fault types\n",
        "            fault_types = ['healthy', 'gain', 'offset', 'stuck-at', 'noise', 'drift', 'hard-over', 'delay-time', 'spike']\n",
        "\n",
        "            # Plotting precision, recall, and F1-score\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            bar_width = 0.2\n",
        "            index = list(range(len(fault_types)))\n",
        "            plt.bar(index, precision, bar_width, label='Precision')\n",
        "            plt.bar([i + bar_width for i in index], recall, bar_width, label='Recall')\n",
        "            plt.bar([i + 2 * bar_width for i in index], f1, bar_width, label='F1-score')\n",
        "            plt.xlabel('Fault Types')\n",
        "            plt.ylabel('Scores')\n",
        "            plt.title('Precision, Recall, and F1-score')\n",
        "            plt.xticks([i + bar_width for i in index], fault_types, rotation=45)\n",
        "            plt.legend()\n",
        "            plt.tight_layout()\n",
        "            plt.grid(axis='y')\n",
        "            plt.savefig(os.path.join(self.graphs_path, 'metrics_combined.png'))\n",
        "            plt.close()\n",
        "\n",
        "# Define hypermodel\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    \n",
        "    cnn_layers = hp.Int('cnn_layers', min_value=0, max_value=5, default=5)\n",
        "    lstm_layers = hp.Int('lstm_layers', min_value=0, max_value=5, default=4)\n",
        "    dense_layers = hp.Int('dense_layers', min_value=0, max_value=5, default=0)\n",
        "    epochs = hp.Int('epochs', min_value=50, max_value=900, default=850)\n",
        "    max_pooling = hp.Int('max_pooling', min_value=0, max_value=1, default=1)\n",
        "    dropout = hp.Int('dropout', min_value=0, max_value=2, default=0)\n",
        "    batch_norm = hp.Int('batch_norm', min_value=0, max_value=2, default=2)\n",
        "    batch_size = hp.Int('batch_size', min_value=64, max_value=150, default=64)\n",
        "    learning_rate = hp.Float('learning_rate', min_value=0.0001, max_value=0.001, default=0.0005, sampling='linear')\n",
        "\n",
        "    for i in range(cnn_layers):\n",
        "        model.add(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same', input_shape=(30, 2)))\n",
        "        if batch_norm:\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "    if max_pooling:\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    for i in range(lstm_layers):\n",
        "        model.add(LSTM(units=64, activation='relu', return_sequences=True))\n",
        "        if batch_norm:\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    for i in range(dense_layers):\n",
        "        model.add(Dense(units=64, activation='relu'))\n",
        "        if dropout:\n",
        "            model.add(Dropout(0.5))\n",
        "        if batch_norm:\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(units=9, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define ALL_CLASSES\n",
        "ALL_CLASSES = {\n",
        "    'Activation': Activation,\n",
        "    'Conv1D': Conv1D,\n",
        "    'BatchNormalization': BatchNormalization,\n",
        "    'MaxPooling1D': MaxPooling1D,\n",
        "    'LSTM': LSTM,\n",
        "    'Flatten': Flatten,\n",
        "    'Dense': Dense,\n",
        "    'Dropout': Dropout\n",
        "}\n",
        "\n",
        "# Define REINFORCE algorithm\n",
        "def reinforce_update(current_hp, rewards, learning_rate=0.001):\n",
        "    rewards -= np.mean(rewards)\n",
        "    rewards /= np.std(rewards)\n",
        "    \n",
        "    for hp_name, reward in zip(current_hp.keys(), rewards):\n",
        "        current_hp[hp_name] += learning_rate * reward\n",
        "    \n",
        "    return current_hp\n",
        "\n",
        "# Define parameters\n",
        "num_episodes = 100\n",
        "max_steps_per_episode = 50\n",
        "current_hyperparameters = {\n",
        "    'cnn_layers': 5,\n",
        "    'lstm_layers': 4,\n",
        "    'dense_layers': 0,\n",
        "    'epochs': 850,\n",
        "    'max_pooling': 1,\n",
        "    'dropout': 0,\n",
        "    'batch_norm': 2,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.0005\n",
        "}\n",
        "\n",
        "# Define metrics callback\n",
        "metrics_callback = MetricsCallback((X_val_array, y_val_array), graphs_path)\n",
        "\n",
        "# Training loop with REINFORCE\n",
        "for episode in range(num_episodes):\n",
        "    episode_rewards = []\n",
        "    \n",
        "    for step in range(max_steps_per_episode):\n",
        "        sampled_hyperparameters = {\n",
        "            'cnn_layers': int(np.random.normal(current_hyperparameters['cnn_layers'], 0.1)),\n",
        "            'lstm_layers': int(np.random.normal(current_hyperparameters['lstm_layers'], 0.1)),\n",
        "            'dense_layers': int(np.random.normal(current_hyperparameters['dense_layers'], 0.1)),\n",
        "            'epochs': int(np.random.normal(current_hyperparameters['epochs'], 0.1)),\n",
        "            'max_pooling': int(np.random.normal(current_hyperparameters['max_pooling'], 0.1)),\n",
        "            'dropout': int(np.random.normal(current_hyperparameters['dropout'], 0.1)),\n",
        "            'batch_norm': int(np.random.normal(current_hyperparameters['batch_norm'], 0.1)),\n",
        "            'batch_size': int(np.random.normal(current_hyperparameters['batch_size'], 0.1)),\n",
        "            'learning_rate': np.random.normal(current_hyperparameters['learning_rate'], 0.1)\n",
        "        }\n",
        "        print(\"sampled_hyperparameters:\", sampled_hyperparameters)\n",
        "\n",
        "        # Debugging: Print ALL_CLASSES and sampled_hyperparameters\n",
        "        print(\"ALL_CLASSES:\", ALL_CLASSES)\n",
        "        print(\"sampled_hyperparameters:\", sampled_hyperparameters)\n",
        "\n",
        "        # Create a new instance of HyperParameters\n",
        "        hp = kt.HyperParameters()\n",
        "\n",
        "        # Add configurations to the HyperParameters object\n",
        "        for key, value in sampled_hyperparameters.items():\n",
        "            hp.Fixed(key, value)\n",
        "\n",
        "        # Call from_config() method with the configured HyperParameters object\n",
        "        hp_config = {'space': sampled_hyperparameters}\n",
        "        print(\"config after from_config:\", hp_config)\n",
        "\n",
        "        # Use the configured HyperParameters object when calling build_model()\n",
        "        model = build_model(hp)\n",
        "        \n",
        "        print(\"Model built:\", model)\n",
        "        \n",
        "        history = model.fit(\n",
        "            X_train_array,\n",
        "            y_train_array,\n",
        "            validation_data=(X_val_array, y_val_array),\n",
        "            batch_size=sampled_hyperparameters['batch_size'],\n",
        "            epochs=sampled_hyperparameters['epochs'],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        y_pred = model.predict(X_val_array)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        y_true = np.argmax(y_val_array, axis=1)\n",
        "        precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "        reward = (precision >= 0.9886 and recall >= 0.9890 and f1 >= 0.9888) - 1\n",
        "        \n",
        "        episode_rewards.append(reward)\n",
        "    \n",
        "    current_hyperparameters = reinforce_update(current_hyperparameters, episode_rewards)\n",
        "    print(f\"Episode {episode + 1}: Rewards - {np.mean(episode_rewards)}\")\n",
        "\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = kt.HyperParameters().from_config(current_hyperparameters)\n",
        "\n",
        "# Build the model with the best hyperparameters\n",
        "best_model = build_model(best_hps)\n",
        "\n",
        "# Train the model\n",
        "history = best_model.fit(\n",
        "    X_train_array,\n",
        "    y_train_array,\n",
        "    validation_data=(X_val_array, y_val_array),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    epochs=best_hps.get('epochs'),\n",
        "    callbacks=[metrics_callback]\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save(os.path.join(best_model_path, 'best_model.h5'))\n",
        "\n",
        "# Calculate metrics on the validation set\n",
        "y_pred = best_model.predict(X_val_array)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_val_array, axis=1)\n",
        "\n",
        "precision = precision_score(y_true, y_pred_classes, average=None, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred_classes, average=None)\n",
        "f1 = f1_score(y_true, y_pred_classes, average=None)\n",
        "\n",
        "# Sensor fault types\n",
        "fault_types = ['healthy', 'gain', 'offset', 'stuck-at', 'noise', 'drift', 'hard-over', 'delay-time', 'spike']\n",
        "\n",
        "# Plotting precision, recall, and F1-score\n",
        "plt.figure(figsize=(10, 5))\n",
        "bar_width = 0.2\n",
        "index = list(range(len(fault_types)))\n",
        "plt.bar(index, precision, bar_width, label='Precision')\n",
        "plt.bar([i + bar_width for i in index], recall, bar_width, label='Recall')\n",
        "plt.bar([i + 2 * bar_width for i in index], f1, bar_width, label='F1-score')\n",
        "plt.xlabel('Fault Types')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Precision, Recall, and F1-score')\n",
        "plt.xticks([i + bar_width for i in index], fault_types, rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y')\n",
        "plt.savefig(os.path.join(graphs_path, 'metrics_combined.png'))\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPVnutrJohyyPYjZ76dMzE1",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
