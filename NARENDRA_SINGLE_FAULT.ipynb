{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingT5M/NARENDRA-SINGLE-FAULT/blob/main/NARENDRA_SINGLE_FAULT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import kerastuner as kt\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, BatchNormalization, MaxPooling1D, LSTM, Flatten, Dense, Dropout, Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set paths\n",
        "data_dir = r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET'\n",
        "graphs_path = os.path.join(data_dir, 'graphs')\n",
        "best_model_path = os.path.join(data_dir, 'best_model')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(graphs_path, exist_ok=True)\n",
        "os.makedirs(best_model_path, exist_ok=True)\n",
        "\n",
        "# Read and process data\n",
        "file_paths = [\n",
        "    'rec3_002-rpm-healthy.csv',\n",
        "    'rec3_004-gain-2.csv',\n",
        "    'rec3_007-offset-1000.csv',\n",
        "    'rec3_008-stuck.csv',\n",
        "    'rec3_009-noise.csv',\n",
        "    'rec3_010-drift.csv',\n",
        "    'rec3_010-hard-over.csv',\n",
        "    'rec3_012-delay-2.0.csv',\n",
        "    'rec3_012-spike.csv'\n",
        "]\n",
        "\n",
        "def process_file(file_path):\n",
        "    df = pd.read_csv(os.path.join(data_dir, file_path))\n",
        "    df['time'] = df['time'].round(3)\n",
        "    threshold_index = df[df['time'] > 300.66].index.min()\n",
        "    if not pd.isnull(threshold_index):\n",
        "        df = df.loc[:threshold_index]\n",
        "    df.to_csv(os.path.join(data_dir, file_path), index=False)\n",
        "\n",
        "for file_path in file_paths:\n",
        "    process_file(file_path)\n",
        "\n",
        "# Concatenate data\n",
        "dfs = [pd.read_csv(os.path.join(data_dir, file_path)) for file_path in file_paths]\n",
        "concatenated_df = pd.concat(dfs, ignore_index=True).dropna().sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Split data\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "train_end = int(len(concatenated_df) * train_ratio)\n",
        "val_end = int(len(concatenated_df) * (train_ratio + val_ratio))\n",
        "\n",
        "train_df = concatenated_df[:train_end]\n",
        "val_df = concatenated_df[train_end:val_end]\n",
        "test_df = concatenated_df[val_end:]\n",
        "\n",
        "# Extract features and labels\n",
        "def extract_features_labels(df):\n",
        "    X = df.iloc[:, :2].values\n",
        "    y = df.iloc[:, 2].values\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = extract_features_labels(train_df)\n",
        "X_val, y_val = extract_features_labels(val_df)\n",
        "X_test, y_test = extract_features_labels(test_df)\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_val_encoded = encoder.transform(y_val.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Create sequences of 30\n",
        "def create_sequences(X, y):\n",
        "    sequence_length = 30\n",
        "    X_sequences = [X[i:i+sequence_length] for i in range(len(X) - sequence_length + 1)]\n",
        "    y_sequences = [y[i+sequence_length-1] for i in range(len(y) - sequence_length + 1)]\n",
        "    return np.array(X_sequences), np.array(y_sequences)\n",
        "\n",
        "X_train_array, y_train_array = create_sequences(X_train_scaled, y_train_encoded)\n",
        "X_val_array, y_val_array = create_sequences(X_val_scaled, y_val_encoded)\n",
        "X_test_array, y_test_array = create_sequences(X_test_scaled, y_test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define metrics callback\n",
        "class MetricsCallback(Callback):\n",
        "    def __init__(self, validation_data, graphs_path):\n",
        "        super(MetricsCallback, self).__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.graphs_path = graphs_path\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % 10 == 0:  # Print validation loss and accuracy every ten minutes\n",
        "            X_val, y_val = self.validation_data\n",
        "            loss, accuracy = self.model.evaluate(X_val, y_val, verbose=0)\n",
        "            print(f\"Validation Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Modify the build_model function to specify the data type directly\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    \n",
        "    cnn_layers = hp.Int('cnn_layers', min_value=0, max_value=5, default=5)\n",
        "    lstm_layers = hp.Int('lstm_layers', min_value=0, max_value=5, default=4)\n",
        "    dense_layers = hp.Int('dense_layers', min_value=0, max_value=5, default=0)\n",
        "    epochs = hp.Int('epochs', min_value=50, max_value=900, default=850)\n",
        "    max_pooling = hp.Int('max_pooling', min_value=0, max_value=1, default=1)\n",
        "    dropout = hp.Int('dropout', min_value=0, max_value=2, default=0)\n",
        "    batch_norm = hp.Int('batch_norm', min_value=0, max_value=2, default=2)\n",
        "    batch_size = hp.Int('batch_size', min_value=64, max_value=150, default=64)\n",
        "    learning_rate = hp.Float('learning_rate', min_value=0.0001, max_value=0.001, default=0.0005, sampling='linear')\n",
        "\n",
        "    for i in range(cnn_layers):\n",
        "        model.add(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same', input_shape=(30, 2), dtype='float32'))\n",
        "        if batch_norm:\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "    if max_pooling:\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    for i in range(lstm_layers):\n",
        "        model.add(LSTM(units=64, activation='relu', return_sequences=True, dtype='float32'))\n",
        "        if batch_norm:\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    for i in range(dense_layers):\n",
        "        model.add(Dense(units=64, activation='relu', dtype='float32'))\n",
        "        if dropout:\n",
        "            model.add(Dropout(0.5))\n",
        "        if batch_norm:\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(units=9, activation='softmax', dtype='float32'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define ALL_CLASSES\n",
        "ALL_CLASSES = {\n",
        "    'Activation': Activation,\n",
        "    'Conv1D': Conv1D,\n",
        "    'BatchNormalization': BatchNormalization,\n",
        "    'MaxPooling1D': MaxPooling1D,\n",
        "    'LSTM': LSTM,\n",
        "    'Flatten': Flatten,\n",
        "    'Dense': Dense,\n",
        "    'Dropout': Dropout\n",
        "}\n",
        "\n",
        "# Define PPO algorithm\n",
        "def ppo_update(current_hp, rewards, learning_rate=0.001):\n",
        "    rewards -= np.mean(rewards)\n",
        "    rewards /= np.std(rewards)\n",
        "    \n",
        "    for hp_name, reward in zip(current_hp.keys(), rewards):\n",
        "        current_hp[hp_name] += learning_rate * reward\n",
        "    \n",
        "    return current_hp\n",
        "\n",
        "\n",
        "# Define parameters\n",
        "num_episodes = 100\n",
        "max_steps_per_episode = 50\n",
        "current_hyperparameters = {\n",
        "    'cnn_layers': 5,\n",
        "    'lstm_layers': 4,\n",
        "    'dense_layers': 0,\n",
        "    'epochs': 850,\n",
        "    'max_pooling': 1,\n",
        "    'dropout': 0,\n",
        "    'batch_norm': 2,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.0005\n",
        "}\n",
        "\n",
        "# Define metrics callback\n",
        "metrics_callback = MetricsCallback((X_val_array, y_val_array), graphs_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reinforcement Learning Starts\n",
            "ALL_CLASSES: {'Activation': <class 'keras.src.layers.core.activation.Activation'>, 'Conv1D': <class 'keras.src.layers.convolutional.conv1d.Conv1D'>, 'BatchNormalization': <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>, 'MaxPooling1D': <class 'keras.src.layers.pooling.max_pooling1d.MaxPooling1D'>, 'LSTM': <class 'keras.src.layers.rnn.lstm.LSTM'>, 'Flatten': <class 'keras.src.layers.reshaping.flatten.Flatten'>, 'Dense': <class 'keras.src.layers.core.dense.Dense'>, 'Dropout': <class 'keras.src.layers.regularization.dropout.Dropout'>}\n",
            "sampled_hyperparameters: {'cnn_layers': 4, 'lstm_layers': 3, 'dense_layers': 0, 'epochs': 849, 'max_pooling': 1, 'dropout': 0, 'batch_norm': 1, 'batch_size': 63, 'learning_rate': 0.06535242581463312}\n",
            "WARNING:tensorflow:From c:\\Users\\T5M\\anaconda3\\envs\\Sankara-Ai\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model built: <keras.src.engine.sequential.Sequential object at 0x0000021F5EBE5010>\n"
          ]
        }
      ],
      "source": [
        "print(\"Reinforcement Learning Starts\")  # Inform about the start of reinforcement learning\n",
        "\n",
        "# Training loop with PPO\n",
        "for episode in range(num_episodes):\n",
        "    episode_rewards = []\n",
        "    \n",
        "    for step in range(max_steps_per_episode):\n",
        "        sampled_hyperparameters = {\n",
        "            'cnn_layers': int(np.random.normal(current_hyperparameters['cnn_layers'], 0.1)),\n",
        "            'lstm_layers': int(np.random.normal(current_hyperparameters['lstm_layers'], 0.1)),\n",
        "            'dense_layers': int(np.random.normal(current_hyperparameters['dense_layers'], 0.1)),\n",
        "            'epochs': int(np.random.normal(current_hyperparameters['epochs'], 0.1)),\n",
        "            'max_pooling': int(np.random.normal(current_hyperparameters['max_pooling'], 0.1)),\n",
        "            'dropout': int(np.random.normal(current_hyperparameters['dropout'], 0.1)),\n",
        "            'batch_norm': int(np.random.normal(current_hyperparameters['batch_norm'], 0.1)),\n",
        "            'batch_size': int(np.random.normal(current_hyperparameters['batch_size'], 0.1)),\n",
        "            'learning_rate': np.random.normal(current_hyperparameters['learning_rate'], 0.1)\n",
        "        }\n",
        "\n",
        "        # Debugging: Print ALL_CLASSES and sampled_hyperparameters\n",
        "        print(\"ALL_CLASSES:\", ALL_CLASSES)\n",
        "        print(\"sampled_hyperparameters:\", sampled_hyperparameters)\n",
        "\n",
        "        # Create a new instance of HyperParameters\n",
        "        hp = kt.HyperParameters()\n",
        "\n",
        "        # Add configurations to the HyperParameters object\n",
        "        for key, value in sampled_hyperparameters.items():\n",
        "            hp.Fixed(key, value)\n",
        "\n",
        "        # Use the configured HyperParameters object when calling build_model()\n",
        "        model = build_model(hp)\n",
        "        \n",
        "        print(\"Model built:\", model)\n",
        "        \n",
        "        history = model.fit(\n",
        "            X_train_array,\n",
        "            y_train_array,\n",
        "            validation_data=(X_val_array, y_val_array),\n",
        "            batch_size=sampled_hyperparameters['batch_size'],\n",
        "            epochs=sampled_hyperparameters['epochs'],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        y_pred = model.predict(X_val_array)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        y_true = np.argmax(y_val_array, axis=1)\n",
        "        precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "        reward = (precision >= 0.9886 and recall >= 0.9890 and f1 >= 0.9888) - 1\n",
        "        \n",
        "        episode_rewards.append(reward)\n",
        "    \n",
        "    current_hyperparameters = ppo_update(current_hyperparameters, episode_rewards)\n",
        "    print(f\"Episode {episode + 1}: Rewards - {np.mean(episode_rewards)}\")\n",
        "\n",
        "print(\"Reinforcement Learning Stops\")  # Inform about the stop of reinforcement learning\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = kt.HyperParameters().from_config(current_hyperparameters)\n",
        "\n",
        "# Build the model with the best hyperparameters\n",
        "best_model = build_model(best_hps)\n",
        "\n",
        "# Train the model\n",
        "history = best_model.fit(\n",
        "    X_train_array,\n",
        "    y_train_array,\n",
        "    validation_data=(X_val_array, y_val_array),\n",
        "    epochs=best_hps.get('epochs'),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    verbose=1,\n",
        "    callbacks=[metrics_callback]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test_array, y_test_array, verbose=0)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "best_model.save(os.path.join(best_model_path, 'best_model.h5'))\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.savefig(os.path.join(graphs_path, 'training_history.png'))\n",
        "plt.show()\n",
        "\n",
        "# Calculate metrics on the validation set\n",
        "y_pred = best_model.predict(X_val_array)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_val_array, axis=1)\n",
        "\n",
        "precision = precision_score(y_true, y_pred_classes, average=None, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred_classes, average=None)\n",
        "f1 = f1_score(y_true, y_pred_classes, average=None)\n",
        "\n",
        "# Sensor fault types\n",
        "fault_types = ['healthy', 'gain', 'offset', 'stuck-at', 'noise', 'drift', 'hard-over', 'delay-time', 'spike']\n",
        "\n",
        "# Plotting precision, recall, and F1-score\n",
        "plt.figure(figsize=(10, 5))\n",
        "bar_width = 0.2\n",
        "index = np.arange(len(fault_types))\n",
        "plt.bar(index, precision, bar_width, label='Precision')\n",
        "plt.bar(index + bar_width, recall, bar_width, label='Recall')\n",
        "plt.bar(index + 2 * bar_width, f1, bar_width, label='F1-score')\n",
        "plt.xlabel('Fault Types')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Performance Metrics')\n",
        "plt.xticks(index + bar_width, fault_types, rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(graphs_path, 'performance_metrics.png'))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPVnutrJohyyPYjZ76dMzE1",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
