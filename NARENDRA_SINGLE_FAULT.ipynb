{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingT5M/NARENDRA-SINGLE-FAULT/blob/main/NARENDRA_SINGLE_FAULT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N2wdO6zaHm7"
      },
      "outputs": [],
      "source": [
        "#import packages\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import keras_tuner as kt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from scipy.stats import randint, uniform \n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, BatchNormalization, MaxPooling1D, LSTM, Flatten, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import Callback\n",
        "from kerastuner import HyperModel, Hyperband\n",
        "# Data is stored in CSV files\n",
        "healthy_data_path = r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_002-rpm-healthy.csv'\n",
        "gain_fault_path = r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_004-gain-2.csv'\n",
        "offset_fault_path = r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_007-offset-1000.csv'\n",
        "stuck_fault_path = r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_008-stuck.csv'\n",
        "noise_fault_path = r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_009-noise.csv'\n",
        "drift_fault_path = r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_010-drift.csv'\n",
        "hard_over_fault_path = r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_010-hard-over.csv'\n",
        "delay_fault_path = r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_012-delay-2.0.csv'\n",
        "spike_fault_path = r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_012-spike.csv'\n",
        "\n",
        "# Make sure time column has 3 decimal places and all data is upto a uniform time stamp\n",
        "def process_file(file_path):\n",
        "    # Read CSV file into a pandas DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Round off values in the first column to 3 decimal places\n",
        "    df['time'] = df['time'].round(3)\n",
        "\n",
        "    # Find the index where the 'time' column is greater than 300.66\n",
        "    threshold_index = df[df['time'] > 300.66].index.min()\n",
        "\n",
        "    # If the threshold is found, delete rows after it\n",
        "    if not pd.isnull(threshold_index):\n",
        "        df = df.loc[:threshold_index]\n",
        "\n",
        "    # Save the modified DataFrame back to the CSV file\n",
        "    df.to_csv(file_path, index=False)\n",
        "\n",
        "# Apply the function to each file path\n",
        "process_file(healthy_data_path)\n",
        "process_file(gain_fault_path)\n",
        "process_file(spike_fault_path)\n",
        "process_file(hard_over_fault_path)\n",
        "process_file(noise_fault_path)\n",
        "\n",
        "# Visualize data\n",
        "def visualize_data(file_path, fault_type):\n",
        "    # Read the CSV file into a DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Plot 'Engine-RPM'\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(df['time'], label='time', color='blue')\n",
        "    plt.title(f'time Plot - {fault_type}')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(df['Engine-RPM'], label='Engine-RPM', color='orange')\n",
        "    plt.title(f'Engine-RPM Plot - {fault_type}')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize data for each file\n",
        "for file_path, fault_type in [(healthy_data_path, 'healthy'), (gain_fault_path, 'gain'),\n",
        "                              (spike_fault_path, 'spike'),\n",
        "                              (hard_over_fault_path, 'hard over'),\n",
        "                              (noise_fault_path, 'noise')]:\n",
        "    visualize_data(file_path, fault_type)\n",
        "    \n",
        "\n",
        "# File paths\n",
        "file_paths = [\n",
        "    r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_002-rpm-healthy.csv',\n",
        "    r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_004-gain-2.csv',\n",
        "    r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_007-offset-1000.csv',\n",
        "    r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_008-stuck.csv',\n",
        "    r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_009-noise.csv',\n",
        "    r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_010-drift.csv',\n",
        "    r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_010-hard-over.csv',\n",
        "    r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_012-delay-2.0.csv',\n",
        "    r'C:\\Users\\T5M\\Desktop\\NARENDRA-SINGLE FAULT\\FAULT DATASET\\rec3_012-spike.csv'\n",
        "]\n",
        "\n",
        "# Fault types\n",
        "fault_types = [\n",
        "    'healthy', 'gain', 'offset', 'stuck-at', 'noise', 'drift', 'hard-over', 'delay-time', 'spike'\n",
        "]\n",
        "\n",
        "# Iterate over each file\n",
        "for file_path, fault_type in zip(file_paths, fault_types):\n",
        "    # Read CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "    \n",
        "    # Add Fault Type column and fill with respective data\n",
        "    df['Fault Type'] = fault_type\n",
        "    \n",
        "    # Write back to CSV\n",
        "    df.to_csv(file_path, index=False)\n",
        "\n",
        "    print(f\"Fault Type column added to {file_path} and filled with {fault_type} data.\")\n",
        "\n",
        "# List to store DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Read each CSV file and append to the list\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "concatenated_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Display the concatenated DataFrame\n",
        "print(concatenated_df)\n",
        "\n",
        "# Drop missing values\n",
        "concatenated_df.dropna(inplace=True)\n",
        "\n",
        "# Mix up the data by shuffling the DataFrame\n",
        "concatenated_df = concatenated_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Extract unique fault types\n",
        "unique_fault_types = concatenated_df['Fault Type'].unique()\n",
        "\n",
        "# Split the DataFrame into training, validation, and testing sets\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "train_end = int(len(concatenated_df) * train_ratio)\n",
        "val_end = int(len(concatenated_df) * (train_ratio + val_ratio))\n",
        "\n",
        "train_df = concatenated_df[:train_end]\n",
        "val_df = concatenated_df[train_end:val_end]\n",
        "test_df = concatenated_df[val_end:]\n",
        "\n",
        "# Ensure all fault types are represented in each set\n",
        "for fault_type in unique_fault_types:\n",
        "    train_fault_type = train_df[train_df['Fault Type'] == fault_type]\n",
        "    val_fault_type = val_df[val_df['Fault Type'] == fault_type]\n",
        "    test_fault_type = test_df[test_df['Fault Type'] == fault_type]\n",
        "\n",
        "    if len(train_fault_type) == 0 or len(val_fault_type) == 0 or len(test_fault_type) == 0:\n",
        "        raise ValueError(\"Fault type '{}' not present in all datasets.\".format(fault_type))\n",
        "\n",
        "# Extract features and labels\n",
        "X_train = train_df.iloc[:, :2].values  # Features\n",
        "y_train = train_df.iloc[:, 2].values   # Labels\n",
        "\n",
        "X_val = val_df.iloc[:, :2].values  # Features\n",
        "y_val = val_df.iloc[:, 2].values   # Labels\n",
        "\n",
        "X_test = test_df.iloc[:, :2].values  # Features\n",
        "y_test = test_df.iloc[:, 2].values   # Labels\n",
        "\n",
        "# Normalize the features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_val_encoded = encoder.transform(y_val.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Create sequences of 30\n",
        "sequence_length = 30\n",
        "X_train_sequences = []\n",
        "y_train_sequences = []\n",
        "for i in range(len(X_train_scaled) - sequence_length + 1):\n",
        "    X_train_sequences.append(X_train_scaled[i:i+sequence_length])\n",
        "    y_train_sequences.append(y_train_encoded[i+sequence_length-1])\n",
        "\n",
        "X_val_sequences = []\n",
        "y_val_sequences = []\n",
        "for i in range(len(X_val_scaled) - sequence_length + 1):\n",
        "    X_val_sequences.append(X_val_scaled[i:i+sequence_length])\n",
        "    y_val_sequences.append(y_val_encoded[i+sequence_length-1])\n",
        "\n",
        "X_test_sequences = []\n",
        "y_test_sequences = []\n",
        "for i in range(len(X_test_scaled) - sequence_length + 1):\n",
        "    X_test_sequences.append(X_test_scaled[i:i+sequence_length])\n",
        "    y_test_sequences.append(y_test_encoded[i+sequence_length-1])\n",
        "\n",
        "# Convert sequences into arrays\n",
        "X_train_array = np.array(X_train_sequences)\n",
        "y_train_array = np.array(y_train_sequences)\n",
        "\n",
        "X_val_array = np.array(X_val_sequences)\n",
        "y_val_array = np.array(y_val_sequences)\n",
        "\n",
        "X_test_array = np.array(X_test_sequences)\n",
        "y_test_array = np.array(y_test_sequences)\n",
        "\n",
        "\n",
        "print(\"Preprocessing completed successfully.\")\n",
        "print(\"Shape of Training Features:\", X_train_array.shape)\n",
        "print(\"Shape of Training Labels:\", y_train_array.shape)\n",
        "print(\"Shape of Validation Features:\", X_val_array.shape)\n",
        "print(\"Shape of Validation Labels:\", y_val_array.shape)\n",
        "print(\"Shape of Testing Features:\", X_test_array.shape)\n",
        "print(\"Shape of Testing Labels:\", y_test_array.shape)\n",
        "\n",
        "# Define hypermodel\n",
        "class CustomHyperModel(HyperModel):\n",
        "    def __init__(self, X_train, y_train, X_val, y_val):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Hyperparameters\n",
        "        cnn_layers = hp.Int('cnn_layers', min_value=0, max_value=5, default=5)\n",
        "        lstm_layers = hp.Int('lstm_layers', min_value=0, max_value=5, default=4)\n",
        "        dense_layers = hp.Int('dense_layers', min_value=0, max_value=5, default=0)\n",
        "        epochs = hp.Int('epochs', min_value=50, max_value=900, default=850)\n",
        "        max_pooling = hp.Int('max_pooling', min_value=0, max_value=1, default=1)\n",
        "        dropout = hp.Int('dropout', min_value=0, max_value=2, default=0)\n",
        "        batch_norm = hp.Int('batch_norm', min_value=0, max_value=2, default=2)\n",
        "        batch_size = hp.Int('batch_size', min_value=64, max_value=150, default=64)\n",
        "        learning_rate = hp.Float('learning_rate', min_value=0.0001, max_value=0.001, default=0.0005, sampling='linear')\n",
        "\n",
        "        # CNN Layers\n",
        "        for i in range(cnn_layers):\n",
        "            model.add(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same', input_shape=(30, 2)))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        # Max Pooling Layer\n",
        "        if max_pooling:\n",
        "            model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "        # LSTM Layers\n",
        "        for i in range(lstm_layers):\n",
        "            model.add(LSTM(units=64, activation='relu', return_sequences=True))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        # Flatten Layer\n",
        "        model.add(Flatten())\n",
        "\n",
        "        # Dense Layers\n",
        "        for i in range(dense_layers):\n",
        "            model.add(Dense(units=64, activation='relu'))\n",
        "            if dropout:\n",
        "                model.add(Dropout(0.5))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        # Output Layer\n",
        "        model.add(Dense(units=9, activation='softmax'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "# Define metrics callback\n",
        "class MetricsCallback(Callback):\n",
        "    def __init__(self, validation_data, graphs_path):\n",
        "        super(MetricsCallback, self).__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.graphs_path = graphs_path\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        X_val, y_val = self.validation_data\n",
        "        y_pred = self.model.predict(X_val)\n",
        "        y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "        y_true = tf.argmax(y_val, axis=1)\n",
        "\n",
        "        precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "        print(f\"Validation Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "        if precision >= 0.9886 and recall >= 0.9890 and f1 >= 0.9888:\n",
        "            print(\"Achieved desired metrics. Stopping training.\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "            # Plotting precision, recall, and F1-score\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            bar_width = 0.2\n",
        "            index = list(range(len(fault_types)))\n",
        "            plt.bar(index, precision, bar_width, label='Precision')\n",
        "            plt.bar([i + bar_width for i in index], recall, bar_width, label='Recall')\n",
        "            plt.bar([i + 2 * bar_width for i in index], f1, bar_width, label='F1-score')\n",
        "            plt.xlabel('Fault Types')\n",
        "            plt.ylabel('Scores')\n",
        "            plt.title('Precision, Recall, and F1-score')\n",
        "            plt.xticks([i + bar_width for i in index], fault_types, rotation=45)\n",
        "            plt.legend()\n",
        "            plt.tight_layout()\n",
        "            plt.grid(axis='y')\n",
        "            plt.savefig(os.path.join(self.graphs_path, 'metrics_combined.png'))\n",
        "            plt.close()\n",
        "\n",
        "# Define paths\n",
        "graphs_path = 'C:/Users/T5M/Desktop/NARENDRA-SINGLE FAULT/NARENDRA-SINGLE-FAULT/graphs'\n",
        "best_model_path = 'C:/Users/T5M/Desktop/NARENDRA-SINGLE FAULT/NARENDRA-SINGLE-FAULT/best_model'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(graphs_path, exist_ok=True)\n",
        "os.makedirs(best_model_path, exist_ok=True)\n",
        "\n",
        "# Define hypermodel\n",
        "hypermodel = CustomHyperModel(X_train_array, y_train_array, X_val_array, y_val_array)\n",
        "\n",
        "# Define the tuner\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=50,\n",
        "    factor=3,\n",
        "    directory='hyperparameters_tuning',\n",
        "    project_name='fault_detection'\n",
        ")\n",
        "\n",
        "# Define metrics callback\n",
        "metrics_callback = MetricsCallback((X_val_array, y_val_array), graphs_path)\n",
        "\n",
        "# Perform the hyperparameter search\n",
        "tuner.search(\n",
        "    X_train_array,\n",
        "    y_train_array,\n",
        "    validation_data=(X_val_array, y_val_array),\n",
        "    callbacks=[metrics_callback],\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best hyperparameters found: {best_hps}\")\n",
        "\n",
        "# Build the model with the best hyperparameters\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the model\n",
        "history = best_model.fit(\n",
        "    X_train_array,\n",
        "    y_train_array,\n",
        "    validation_data=(X_val_array, y_val_array),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    epochs=best_hps.get('epochs'),\n",
        "    callbacks=[metrics_callback]\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save(os.path.join(best_model_path, 'best_model.h5'))\n",
        "\n",
        "# Calculate metrics on the validation set\n",
        "y_pred = best_model.predict(X_val_array)\n",
        "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "y_true = tf.argmax(y_val_array, axis=1)\n",
        "\n",
        "precision = precision_score(y_true, y_pred_classes, average=None, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred_classes, average=None)\n",
        "f1 = f1_score(y_true, y_pred_classes, average=None)\n",
        "\n",
        "# Sensor fault types\n",
        "fault_types = ['healthy', 'gain', 'offset', 'stuck-at', 'noise', 'drift', 'hard-over', 'delay-time', 'spike']\n",
        "\n",
        "# Plotting precision, recall, and F1-score\n",
        "plt.figure(figsize=(10, 5))\n",
        "bar_width = 0.2\n",
        "index = list(range(len(fault_types)))\n",
        "plt.bar(index, precision, bar_width, label='Precision')\n",
        "plt.bar([i + bar_width for i in index], recall, bar_width, label='Recall')\n",
        "plt.bar([i + 2 * bar_width for i in index], f1, bar_width, label='F1-score')\n",
        "plt.xlabel('Fault Types')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Precision, Recall, and F1-score')\n",
        "plt.xticks([i + bar_width for i in index], fault_types, rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y')\n",
        "plt.savefig(os.path.join(graphs_path, 'metrics_combined.png'))\n",
        "plt.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPVnutrJohyyPYjZ76dMzE1",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
